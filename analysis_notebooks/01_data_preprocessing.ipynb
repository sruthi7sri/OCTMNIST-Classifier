{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c652efb3-faf6-4075-aea0-a85130665007",
   "metadata": {},
   "source": [
    "# PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ed6acf-26b9-4517-85d1-383a9b77002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sruthisriv/miniforge3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef705e9-f0b5-41fe-89b6-cba176e457c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314f6319-5d93-453d-9c5d-c58cea42cde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "#creating 2D tensor with 5 rows and 3 cols\n",
    "z = torch.zeros(5,3)\n",
    "print(z)\n",
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c777e897-3361-435a-a1d7-5a4aabdc14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "z = torch.ones((5,3), dtype = torch.int16)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4644e579-f4c2-49fe-9a83-888135e6112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random tensor\n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736],\n",
      "        [0.4216, 0.0691]])\n",
      "\n",
      " Another different random tensor\n",
      "tensor([[0.2332, 0.4047],\n",
      "        [0.2162, 0.9927],\n",
      "        [0.4128, 0.5938]])\n",
      "\n",
      " Should match r1: \n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736],\n",
      "        [0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "r1 = torch.rand(3,2)\n",
    "print(\"A random tensor\")\n",
    "print(r1)\n",
    "\n",
    "r2 = torch.rand(3,2)\n",
    "print(\"\\n Another different random tensor\")\n",
    "print(r2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "r3 = torch.rand(3,2)\n",
    "print(\"\\n Should match r1: \")\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12497214-dc7c-4c03-9b14-1300f50f0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m r1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     12\u001b[0m r2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m r3 \u001b[38;5;241m=\u001b[39m \u001b[43mr1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(2,3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2,3)*2\n",
    "print(twos)\n",
    "\n",
    "threes = ones+twos\n",
    "print(threes)\n",
    "print(threes.shape)\n",
    "\n",
    "r1 = torch.rand(2,3)\n",
    "r2 = torch.rand(3,2)\n",
    "r3 = r1 + r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "608c3267-705d-4f6a-89c6-5d188541d481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random matrix, r:\n",
      "tensor([[-0.3071, -0.8297],\n",
      "        [-0.8616, -0.5241]])\n",
      "\n",
      "Abs val of r\n",
      "tensor([[0.3071, 0.8297],\n",
      "        [0.8616, 0.5241]])\n",
      "\n",
      "Inverse Sine\n",
      "tensor([[-0.3122, -0.9785],\n",
      "        [-1.0383, -0.5516]])\n",
      "\n",
      "Determinant\n",
      "tensor(-0.5538)\n",
      "\n",
      "SVD\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.6461, -0.7633],\n",
      "        [-0.7633,  0.6461]]),\n",
      "S=tensor([1.2684, 0.4366]),\n",
      "V=tensor([[ 0.6749, -0.7379],\n",
      "        [ 0.7379,  0.6749]]))\n",
      "\n",
      "avg and SD\n",
      "(tensor(0.2639), tensor(-0.6306))\n",
      "\n",
      "Max val\n",
      "tensor(-0.3071)\n"
     ]
    }
   ],
   "source": [
    "r = torch.rand(2,2) - 0.5*2\n",
    "print(\"A random matrix, r:\")\n",
    "print(r)\n",
    "\n",
    "# common math ops\n",
    "print(\"\\nAbs val of r\")\n",
    "print(torch.abs(r))\n",
    "\n",
    "#trigo fns\n",
    "print(\"\\nInverse Sine\")\n",
    "print(torch.asin(r))\n",
    "\n",
    "#lin alg - determinant and SVD\n",
    "print(\"\\nDeterminant\")\n",
    "print(torch.det(r))\n",
    "print(\"\\nSVD\")\n",
    "print(torch.svd(r))\n",
    "\n",
    "#statistical and aggregate operations\n",
    "print(\"\\navg and SD\")\n",
    "print(torch.std_mean(r))\n",
    "print(\"\\nMax val\")\n",
    "print(torch.max(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e63bbc-9076-4576-a97b-eadc0712b529",
   "metadata": {},
   "source": [
    "# A simple Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98853a74-b74d-4239-8dee-6d95bb9fd973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                     # for all things PyTorch\n",
    "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
    "import torch.nn.functional as F  # for the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ec78a6-c07d-4ca2-9883-2f51c9784b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 3 input image channels (RGB), 8 output channels, 5x5 convolution kernel\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.conv2 = nn.Conv2d(8, 32, 5)\n",
    "        \n",
    "        # Adjusting dimensions according to new conv layers\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 150)  # Adjusted fully connected layer\n",
    "        self.fc2 = nn.Linear(150, 100)\n",
    "        self.fc3 = nn.Linear(100, 5)  # Final output classes changed to 5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Final output layer\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # Exclude batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec17517d-6429-4343-b0dd-6ccd5b60cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=150, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 3, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[-0.0937,  0.0764, -0.0106,  0.0869,  0.0138]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)\n",
    "\n",
    "# Create an input tensor for a batch of 1 RGB image (3 channels, 32x32)\n",
    "input = torch.rand(1, 3, 32, 32)   \n",
    "print('\\nImage batch shape:')\n",
    "print(input.shape)\n",
    "\n",
    "# Pass the input through the network\n",
    "output = net(input)\n",
    "print('\\nRaw output:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a59227-0812-4b26-9f61-8d8235f3a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"A_Simple_PyTorch_Model_LeNet_model_weights.pth\")\n",
    "print(\"LeNet model weights saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd5ea7-21c9-4a4b-b3fc-c58d0eadf311",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47ba0f7-5ffb-413f-b81a-0337b02e3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b333557d-25f9-4982-9b6d-8d40e80f1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Grayscale images have one channel\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488b8ba4-d94d-41a4-b34e-2e4e5edea5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                             download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f1cdff-c824-4c70-a3ec-9efeae776771",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06d0cf59-76ee-41b0-8a8d-83b7e3e6a79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pullover T-shirt/top      Dress        Bag   Pullover T-shirt/top       Coat Ankle boot\n"
     ]
    }
   ],
   "source": [
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img * 0.5 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')  # Convert to grayscale\n",
    "    plt.axis('off')  # Remove axes for better visualization\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of training data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print corresponding labels\n",
    "print(' '.join('%10s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208e1b0-9d20-4334-a2ef-b889e013fadb",
   "metadata": {},
   "source": [
    "# A simple pyTorch Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63f30319-61f5-4abb-809a-33c8ca511a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d8aa64c-1c86-4098-8bf8-33e6738153a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Adjusted for grayscale images\n",
    "])\n",
    "\n",
    "# Load the FashionMNIST dataset\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "# Define class labels for FashionMNIST\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dab281c-2823-45b9-9d0d-fb8e3afc2f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/top       Coat    Trouser      Dress    Trouser        Bag T-shirt/top      Dress\n"
     ]
    }
   ],
   "source": [
    "# Function to visualize images\n",
    "def imshow(img):\n",
    "    img = img * 0.5 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')  # Convert to grayscale\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(' '.join('%10s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47b9377e-693a-4887-a485-1c36a9e7b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel for grayscale\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Adjusted size after pooling\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)  # Flatten tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize network, loss function, and optimizer\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Using Adam optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e636a5e-461c-47cb-a8ed-f2f7400e10d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1000] loss: 0.820\n",
      "[1, 2000] loss: 0.566\n",
      "[1, 3000] loss: 0.495\n",
      "[1, 4000] loss: 0.454\n",
      "[1, 5000] loss: 0.412\n",
      "[1, 6000] loss: 0.392\n",
      "[1, 7000] loss: 0.397\n",
      "[2, 1000] loss: 0.355\n",
      "[2, 2000] loss: 0.348\n",
      "[2, 3000] loss: 0.341\n",
      "[2, 4000] loss: 0.336\n",
      "[2, 5000] loss: 0.351\n",
      "[2, 6000] loss: 0.342\n",
      "[2, 7000] loss: 0.334\n",
      "[3, 1000] loss: 0.323\n",
      "[3, 2000] loss: 0.303\n",
      "[3, 3000] loss: 0.295\n",
      "[3, 4000] loss: 0.310\n",
      "[3, 5000] loss: 0.318\n",
      "[3, 6000] loss: 0.280\n",
      "[3, 7000] loss: 0.299\n",
      "[4, 1000] loss: 0.276\n",
      "[4, 2000] loss: 0.278\n",
      "[4, 3000] loss: 0.277\n",
      "[4, 4000] loss: 0.286\n",
      "[4, 5000] loss: 0.279\n",
      "[4, 6000] loss: 0.272\n",
      "[4, 7000] loss: 0.279\n",
      "[5, 1000] loss: 0.252\n",
      "[5, 2000] loss: 0.261\n",
      "[5, 3000] loss: 0.254\n",
      "[5, 4000] loss: 0.278\n",
      "[5, 5000] loss: 0.275\n",
      "[5, 6000] loss: 0.258\n",
      "[5, 7000] loss: 0.270\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 999:  # Print loss every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73bc1416-4d75-44f7-a912-ba376fd5b3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 88.46 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed6dff5e-473f-427f-a958-f6c822d90634",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the model weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1_Introduction_to_PyTorch_model_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the model weights\n",
    "torch.save(net.state_dict(), \"1_Introduction_to_PyTorch_net_model_weights.pth\")\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edc8a4-827f-4abc-97c3-5ad7ec20e5ad",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6e61a-2759-4adb-b6b2-069ca14aa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b0212-bd80-4e9b-bb82-84e2fc3c68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038ba9c-8ee5-46bc-8753-de1b77793673",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros(3, 4)\n",
    "print(\"Zeros Tensor:\\n\", zeros)\n",
    "\n",
    "ones = torch.ones(3, 4)\n",
    "print(\"\\nOnes Tensor:\\n\", ones)\n",
    "\n",
    "torch.manual_seed(2025)  \n",
    "random = torch.rand(3, 4) \n",
    "print(\"\\nRandom Tensor:\\n\", random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed0abe-4235-4300-a7b3-3bfa1c255d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2025)\n",
    "random1 = torch.rand(3, 4)  \n",
    "print(\"Random Tensor 1:\\n\", random1)\n",
    "\n",
    "random2 = torch.rand(3, 4)\n",
    "print(\"\\nRandom Tensor 2:\\n\", random2)\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "random3 = torch.rand(3, 4)\n",
    "print(\"\\nRandom Tensor 3 (after resetting seed):\\n\", random3)\n",
    "\n",
    "random4 = torch.rand(3, 4)\n",
    "print(\"\\nRandom Tensor 4:\\n\", random4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334457a8-d373-488c-a754-5f5cab1877db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(3, 3, 4)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c136a24-fa08-4659-81ce-7b391e74b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_constants = torch.tensor([[1.41421, 2.30258], [0.57721, 3.14159]])  \n",
    "print(\"Some Constants:\\n\", some_constants)\n",
    "\n",
    "some_integers = torch.tensor((5, 11, 23, 29, 37, 41, 47, 53))\n",
    "print(\"\\nSome Integers (Primes):\\n\", some_integers)\n",
    "\n",
    "more_integers = torch.tensor(((1, 3, 5), [2, 4, 6]))\n",
    "print(\"\\nMore Integers:\\n\", more_integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da180b-a089-4ed6-b072-d8a831bea8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3, 4), dtype=torch.int32)\n",
    "print(\"Tensor a (Ones, int32):\\n\", a)\n",
    "\n",
    "b = torch.rand((3, 4), dtype=torch.float32) * 50.0\n",
    "print(\"\\nTensor b (Random values scaled to 50, float32):\\n\", b)\n",
    "\n",
    "c = b.to(torch.int64)\n",
    "print(\"\\nTensor c (Converted to int64):\\n\", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed874776-2a7f-478c-a2a0-36e070b6ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.zeros(3, 3) + 3  \n",
    "twos = torch.ones(3, 3) * 4   \n",
    "threes = (torch.ones(3, 3) * 11 - 1) / 2  \n",
    "fours = twos ** 2  \n",
    "sqrt2s = twos ** 0.5  \n",
    "\n",
    "print(\"Ones (Filled with 3s):\\n\", ones)\n",
    "print(\"\\nTwos (Filled with 4s):\\n\", twos)\n",
    "print(\"\\nThrees (Filled with 5s):\\n\", threes)\n",
    "print(\"\\nFours (4^2 = 16):\\n\", fours)\n",
    "print(\"\\nSquare Root of Twos (sqrt(4) = 2):\\n\", sqrt2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3788ba-2a87-4ff1-877b-a2daeb9cce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "powers2 = twos ** torch.tensor([[2, 3, 4], [5, 6, 7], [8, 9, 10]])\n",
    "print(\"Powers of Twos:\\n\", powers2)\n",
    "\n",
    "fives = ones + fours  \n",
    "print(\"\\nFives (5 + 9):\\n\", fives)\n",
    "\n",
    "dozens = threes * fours  \n",
    "print(\"\\nDozens (6 * 9):\\n\", dozens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7097a7-4e8f-47a2-9972-ef2a9bbca32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 2)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38410808-ba02-4334-84f7-0ee4db6b9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(3, 5)\n",
    "doubled = rand * (torch.ones(1, 5) * 3)\n",
    "\n",
    "print(\"Random Tensor:\\n\", rand)\n",
    "print(\"\\nScaled Tensor (Multiplied by 3):\\n\", doubled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74ca44-3661-4210-900c-a78b71d18fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5, 4, 3)\n",
    "b = a * torch.rand(4, 3)\n",
    "print(\"Tensor b (Broadcasted over dim 1):\\n\", b)\n",
    "c = a * torch.rand(4, 1)  \n",
    "print(\"\\nTensor c (Broadcasted over 3rd dim):\\n\", c)\n",
    "d = a * torch.rand(1, 3)  \n",
    "print(\"\\nTensor d (Broadcasted over 2nd dim):\\n\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe144c9-32b4-4f65-91fc-653ed2a3c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
    "\n",
    "c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
    "\n",
    "d = a * torch.rand((0, ))   # can't broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cbd44-3a6a-4e3a-9a6b-9f3325a77347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "a = torch.rand(3, 5) * 4 - 2  \n",
    "print('Common functions:')\n",
    "print(\"Absolute:\\n\", torch.abs(a))\n",
    "print(\"Ceil:\\n\", torch.ceil(a))\n",
    "print(\"Floor:\\n\", torch.floor(a))\n",
    "print(\"Clamped (-1 to 1):\\n\", torch.clamp(a, -1, 1))\n",
    "\n",
    "angles = torch.tensor([0, math.pi / 6, math.pi / 3, 2 * math.pi / 3])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(\"Angles:\\n\", angles)\n",
    "print(\"Sine values:\\n\", sines)\n",
    "print(\"Inverse Sine (Arcsin):\\n\", inverses)\n",
    "\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([3, 12, 25])\n",
    "c = torch.tensor([5, 15, 30])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "e = torch.ones(1, 3)  \n",
    "print(torch.eq(d, e))  \n",
    "\n",
    "print('\\nReduction ops:')\n",
    "print(\"Max value:\\n\", torch.max(d))         \n",
    "print(\"Max value extracted:\\n\", torch.max(d).item())  \n",
    "print(\"Mean (average):\\n\", torch.mean(d))   \n",
    "print(\"Standard deviation:\\n\", torch.std(d)) \n",
    "print(\"Product of all elements:\\n\", torch.prod(d)) \n",
    "print(\"Unique elements:\\n\", torch.unique(torch.tensor([3, 3, 4, 4, 5, 6])))  \n",
    "\n",
    "v1 = torch.tensor([1., 2., 3.])  \n",
    "v2 = torch.tensor([4., 5., 6.])  \n",
    "m1 = torch.rand(3, 3)  \n",
    "m2 = torch.tensor([[2., 0., 0.], [0., 2., 0.], [0., 0., 2.]])  \n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(\"Cross Product v1 x v2:\\n\", torch.cross(v1, v2))\n",
    "print(\"Random Matrix m1:\\n\", m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(\"Matrix Multiplication (m1 * 2I):\\n\", m3)  \n",
    "print(\"SVD Decomposition of m3:\\n\", torch.svd(m3))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a04b82-b6ad-4a1f-bc31-699afcca1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = torch.tensor([0, math.pi / 6, math.pi / 3, 2 * math.pi / 3])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(\"Sine of a (New Tensor):\\n\", torch.sin(a)) \n",
    "print(\"a remains unchanged:\\n\", a)  \n",
    "\n",
    "b = torch.tensor([0, math.pi / 6, math.pi / 3, 2 * math.pi / 3])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(\"Sine of b (In-place operation):\\n\", torch.sin_(b))  \n",
    "print(\"b after in-place operation:\\n\", b)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7947a61-6511-417b-ba09-b1856fba12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 3)  \n",
    "b = torch.rand(3, 3) * 5  \n",
    "\n",
    "print(\"Before:\")\n",
    "print(\"Tensor a:\\n\", a)\n",
    "print(\"Tensor b:\\n\", b)\n",
    "\n",
    "print(\"\\nAfter adding:\")\n",
    "print(a.add_(b)) \n",
    "print(\"Tensor a (modified):\\n\", a)\n",
    "print(\"Tensor b (unchanged by addition):\\n\", b)\n",
    "\n",
    "print(\"\\nAfter multiplying:\")\n",
    "print(b.mul_(b))  \n",
    "print(\"Tensor b (modified):\\n\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa81c3-0575-492d-bd20-e7c6625d5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 3) * 10 \n",
    "b = torch.rand(3, 3) * 5   \n",
    "c = torch.zeros(3, 3)      \n",
    "\n",
    "old_id = id(c)\n",
    "\n",
    "print(\"Initial Tensor c (zeros):\\n\", c)\n",
    "\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(\"\\nTensor c after matmul(a, b, out=c):\\n\", c)  \n",
    "\n",
    "assert c is d  \n",
    "assert id(c) == old_id  \n",
    "\n",
    "torch.rand(3, 3, out=c)  \n",
    "print(\"\\nTensor c after torch.rand(3, 3, out=c):\\n\", c)  \n",
    "\n",
    "assert id(c) == old_id  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c40b04-2a8c-4130-8179-168094431aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 3)\n",
    "b = a  \n",
    "\n",
    "a[1][2] = 999  \n",
    "\n",
    "print(\"Tensor a after modification:\\n\", a)\n",
    "print(\"\\nTensor b (shares memory with a, so it also changes):\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499b216-ab4f-41f9-a836-a57d67520486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = torch.ones(3, 3)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a  \n",
    "print(\"Equality Check (Same Contents but Different Objects):\\n\", torch.eq(a, b))\n",
    "\n",
    "a[1][2] = 777  \n",
    "\n",
    "print(\"\\nTensor a after modification:\\n\", a)\n",
    "print(\"\\nTensor b (Remains Unchanged as an Independent Copy):\\n\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b238e6e-0e9a-46d4-a171-b473726ac1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 3, requires_grad=True) \n",
    "print(\"Tensor a (Original, requires_grad=True):\\n\", a)\n",
    "\n",
    "b = a.clone()\n",
    "print(\"\\nTensor b (Clone of a, still requires_grad=True):\\n\", b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(\"\\nTensor c (Detached & Cloned, requires_grad=False):\\n\", c)\n",
    "\n",
    "print(\"\\nTensor a (After operations, remains unchanged):\\n\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc968593-9e65-446c-8c32-0d043b043d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('We have a GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa2453-d26e-4d6a-9c40-c13d3f7f84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540e7df-ec0e-4580-9b2f-ec625e2de2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022adbd-1710-4dc5-96f1-f86b6ea65d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(3, 2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97bb452-a3de-4f74-82b7-8dbaa38a6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 128, 128)  \n",
    "b = a.unsqueeze(0)\n",
    "print(\"Shape of a (Before unsqueeze):\", a.shape)  \n",
    "print(\"Shape of b (After unsqueeze at dim=0):\", b.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a372e1-a32d-4674-bd27-a19ff2ae8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46f761-d708-4998-9cce-e1f5f099b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 15)  \n",
    "print(\"Shape of a (Before squeeze):\", a.shape)\n",
    "print(\"Tensor a:\\n\", a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(\"\\nShape of b (After squeeze):\", b.shape)  \n",
    "print(\"Tensor b:\\n\", b)\n",
    "\n",
    "c = torch.rand(3, 3)  \n",
    "print(\"\\nShape of c (Before squeeze):\", c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(\"Shape of d (After squeeze, should be the same as c):\", d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7927745-7f18-4d27-baee-74bf56f48680",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5, 4, 3)\n",
    "b = torch.rand(4)  \n",
    "c = b.unsqueeze(1) \n",
    "print(\"Shape of c after unsqueeze:\\n\", c.shape)\n",
    "print(\"\\nResult of a * c:\\n\", a * c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b5ef3-96e5-41d1-aeaa-8cfeecea330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_me = torch.rand(3, 128, 128)\n",
    "print(\"Shape of batch_me (Before unsqueeze):\", batch_me.shape)\n",
    "\n",
    "batch_me.unsqueeze_(0)  \n",
    "print(\"Shape of batch_me (After unsqueeze at dim=0):\", batch_me.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a612d-de8e-46f6-940d-c4bc315cc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3d = torch.rand(8, 15, 15)  \n",
    "print(\"Shape of output3d (Original 3D tensor):\", output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(8 * 15 * 15)\n",
    "print(\"Shape of input1d (After reshape to 1D):\", input1d.shape)\n",
    "\n",
    "print(\"Using torch.reshape:\", torch.reshape(output3d, (8 * 15 * 15,)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f6357-d842-4949-940e-c0b1b943849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((3, 4))\n",
    "print(\"NumPy Array:\\n\", numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(\"\\nPyTorch Tensor (From NumPy Array):\\n\", pytorch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6f149-ded6-43ac-af30-e15f2d0db162",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_rand = torch.rand(3, 4)\n",
    "print(\"\\nPyTorch Random Tensor:\\n\", pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(\"\\nNumPy Array (From PyTorch Random Tensor):\\n\", numpy_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57193555-442c-49fb-b1bf-e791db4d7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array[2, 2] = 99\n",
    "print(\"\\nModified NumPy Array (Reflected in PyTorch Tensor):\\n\", pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 3] = 42\n",
    "print(\"\\nModified PyTorch Tensor (Reflected in NumPy Array):\\n\", numpy_rand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72adeec-3182-4bde-adb9-ecce867d5fc3",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2fbe0-9135-4c4c-805b-f8742a780081",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6b493-b4b8-4567-8a67-770f4d311c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5c11a-ed2d-4678-bbac-c163cff27c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4434b0-c886-448f-837e-35e742ffbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sin(a)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d561e5-2b07-43dd-83cb-81a7c4d5c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d44ce-eb59-4094-ac76-994a4b913de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 3 * b\n",
    "print(c)\n",
    "\n",
    "d = c + 2\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78392b1b-eb74-4aa9-8f95-562dbded7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = d.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3818c-1f82-402e-967b-af57536c4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('d:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7d7e9-02de-405d-bf23-a2299145ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "print(a.grad)\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ba758-2523-42f9-82bf-83ae433cadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DIM_IN = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "DIM_OUT = 10\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(1000, 100)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
    "\n",
    "model = TinyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb68e8-0f73-4582-a024-98dccdbac8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layer2.weight[0][0:10]) # just a small slice\n",
    "print(model.layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4a1d5-e8fd-49ef-aae4-d72a9ab3b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "prediction = model(some_input)\n",
    "\n",
    "loss = (ideal_output - prediction).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b025ac-1a22-4c52-b453-dac0d88d2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799175d-592b-46ba-be52-8fffe4281557",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b012d0-3f83-4af1-b07d-dde231ce6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for i in range(0, 5):\n",
    "    prediction = model(some_input)\n",
    "    loss = (ideal_output - prediction).pow(2).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "if model.layer2.weight.grad is not None:\n",
    "    print(f\"Gradients before zeroing:\\n{model.layer2.weight.grad[0][:10]}\")\n",
    "else:\n",
    "    print(\"Gradients are None. Ensure that loss.backward() has been called.\")\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "if model.layer2.weight.grad is not None:\n",
    "    print(f\"Gradients after zeroing:\\n{model.layer2.weight.grad[0][:10]}\")\n",
    "else:\n",
    "    print(\"Gradients are None after zeroing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f037221-0ff7-4ac3-a575-34c8f8f9093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(TinyModel.state_dict(), \"3_Fundamentals_of_Autograd_model_weights.pth\")\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dae222-3b00-47a0-bef4-6c9e77db5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040a284-01f0-4cfa-b6cc-b00607c8839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print(c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "\n",
    "print(c2)\n",
    "\n",
    "c3 = a * b\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4ba5a-e014-4b2f-91e3-818705116820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print(c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ce4b9-23a7-4389-9806-b23b51a50da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, requires_grad=True)\n",
    "y = x.detach()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2415f-4e2a-4dd9-b838-8e9784473d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "torch.sin_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffc65c-3ff2-4ff1-83de-284aa89c966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    run_on_gpu = True\n",
    "    \n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = (z / x) * y\n",
    "        \n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cf88c-13a4-4735-b9e3-f53a79f876ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417637a0-c7a5-453a-b9ab-3bd50d433860",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ec467-e8f7-4f04-8d4f-ef4a09eaec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_adder(x, y):\n",
    "    return 2 * x.exp() + 3 * y\n",
    "\n",
    "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79c1dd-4c5d-490a-aa4e-73788cc409a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a52a1c-7e64-462c-8d4e-6b8f75d238e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_some_doubling(x):\n",
    "    y = x * 2\n",
    "    while y.data.norm() < 1000:\n",
    "        y = y * 2\n",
    "    return y\n",
    "\n",
    "inputs = torch.randn(3)\n",
    "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
    "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458531af-2f44-4054-95ff-270716a23818",
   "metadata": {},
   "source": [
    "# Building Models in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0929b2fb-0344-4828-b188-1924ebb65c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=256, out_features=64, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0748,  0.0327,  0.0583,  ...,  0.0647, -0.0683,  0.0183],\n",
      "        [ 0.0868,  0.0550, -0.0749,  ..., -0.0054,  0.0654, -0.0051],\n",
      "        [-0.0789,  0.0524,  0.0680,  ...,  0.0266,  0.0134, -0.0852],\n",
      "        ...,\n",
      "        [ 0.0792,  0.0504, -0.0587,  ..., -0.0461,  0.0622, -0.0158],\n",
      "        [ 0.0675, -0.0780, -0.0192,  ..., -0.0528, -0.0005, -0.0473],\n",
      "        [-0.0342, -0.0225,  0.0285,  ..., -0.0624, -0.0499, -0.0038]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0781, -0.0583,  0.0067,  0.0140, -0.0712,  0.0702, -0.0421, -0.0106,\n",
      "         0.0439, -0.0632, -0.0705,  0.0877,  0.0430, -0.0007, -0.0724, -0.0557,\n",
      "        -0.0596,  0.0079, -0.0464, -0.0280, -0.0267,  0.0449,  0.0707,  0.0142,\n",
      "        -0.0551,  0.0134,  0.0812, -0.0129, -0.0494,  0.0415,  0.0605, -0.0209,\n",
      "         0.0711,  0.0138, -0.0490, -0.0727, -0.0487,  0.0278,  0.0671, -0.0476,\n",
      "        -0.0338, -0.0725,  0.0613,  0.0235,  0.0818, -0.0566, -0.0597,  0.0593,\n",
      "        -0.0644, -0.0307,  0.0824, -0.0784, -0.0069,  0.0696, -0.0491, -0.0656,\n",
      "         0.0257, -0.0453, -0.0782, -0.0252,  0.0577,  0.0521, -0.0215,  0.0614,\n",
      "        -0.0337, -0.0485, -0.0019,  0.0838, -0.0378, -0.0054,  0.0068,  0.0501,\n",
      "         0.0150,  0.0615,  0.0602, -0.0574, -0.0048,  0.0820,  0.0062, -0.0681,\n",
      "        -0.0519, -0.0638,  0.0056, -0.0148, -0.0617,  0.0548, -0.0735, -0.0396,\n",
      "        -0.0519, -0.0860,  0.0373, -0.0638,  0.0173,  0.0177, -0.0547,  0.0022,\n",
      "        -0.0590, -0.0496,  0.0736, -0.0880, -0.0570, -0.0368, -0.0577, -0.0645,\n",
      "        -0.0728,  0.0451, -0.0661, -0.0590,  0.0785,  0.0161, -0.0394, -0.0690,\n",
      "         0.0015, -0.0252, -0.0295,  0.0361,  0.0767, -0.0521,  0.0201,  0.0545,\n",
      "         0.0021, -0.0819, -0.0635,  0.0329,  0.0111, -0.0792,  0.0132,  0.0526,\n",
      "        -0.0465,  0.0221,  0.0718, -0.0080, -0.0300,  0.0606,  0.0202,  0.0275,\n",
      "         0.0599,  0.0023,  0.0005, -0.0463, -0.0205, -0.0871, -0.0504,  0.0397,\n",
      "        -0.0826, -0.0099,  0.0408, -0.0585, -0.0745, -0.0378, -0.0005,  0.0445,\n",
      "        -0.0212, -0.0386,  0.0349, -0.0643, -0.0440,  0.0318, -0.0859,  0.0577,\n",
      "         0.0230,  0.0504,  0.0872,  0.0728, -0.0132, -0.0290,  0.0797,  0.0534,\n",
      "         0.0122, -0.0103, -0.0770,  0.0560,  0.0600,  0.0863, -0.0093,  0.0400,\n",
      "         0.0211,  0.0795, -0.0376,  0.0594, -0.0514,  0.0124,  0.0673,  0.0135,\n",
      "        -0.0244,  0.0041, -0.0345,  0.0712, -0.0737, -0.0855, -0.0685,  0.0458,\n",
      "        -0.0652,  0.0298,  0.0620,  0.0365,  0.0053, -0.0758, -0.0035, -0.0604,\n",
      "        -0.0361, -0.0092, -0.0251, -0.0439,  0.0006, -0.0465,  0.0574, -0.0197,\n",
      "        -0.0769, -0.0366,  0.0057, -0.0105,  0.0683, -0.0290, -0.0330,  0.0001,\n",
      "        -0.0140, -0.0733, -0.0493, -0.0813,  0.0151, -0.0004, -0.0183,  0.0344,\n",
      "        -0.0344,  0.0616, -0.0201,  0.0848, -0.0165,  0.0467, -0.0092,  0.0611,\n",
      "         0.0235,  0.0288, -0.0876, -0.0248, -0.0171,  0.0867,  0.0149,  0.0781,\n",
      "        -0.0321, -0.0368, -0.0576,  0.0301, -0.0013,  0.0512,  0.0350,  0.0471,\n",
      "        -0.0363,  0.0521, -0.0302, -0.0492, -0.0772,  0.0235,  0.0809,  0.0822],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0099,  0.0338, -0.0149,  ..., -0.0614,  0.0496,  0.0372],\n",
      "        [-0.0418,  0.0303, -0.0294,  ...,  0.0404,  0.0209, -0.0068],\n",
      "        [ 0.0126, -0.0376,  0.0243,  ..., -0.0406, -0.0408,  0.0189],\n",
      "        ...,\n",
      "        [-0.0349, -0.0559,  0.0267,  ...,  0.0554, -0.0380,  0.0186],\n",
      "        [-0.0174, -0.0222, -0.0576,  ..., -0.0388,  0.0344,  0.0611],\n",
      "        [ 0.0115, -0.0240, -0.0193,  ..., -0.0504, -0.0433, -0.0525]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0476, -0.0205,  0.0407, -0.0515,  0.0580, -0.0602,  0.0106,  0.0336,\n",
      "         0.0288,  0.0452,  0.0413,  0.0099, -0.0441, -0.0404,  0.0541, -0.0079,\n",
      "        -0.0178, -0.0208, -0.0203,  0.0600,  0.0355, -0.0322, -0.0383,  0.0596,\n",
      "        -0.0607,  0.0185, -0.0304,  0.0111, -0.0483, -0.0080,  0.0376, -0.0591,\n",
      "         0.0040,  0.0167,  0.0197, -0.0184,  0.0612,  0.0359,  0.0534, -0.0554,\n",
      "         0.0461,  0.0151,  0.0290, -0.0480, -0.0085,  0.0621,  0.0106,  0.0182,\n",
      "        -0.0346,  0.0053, -0.0373, -0.0020,  0.0012,  0.0160,  0.0402, -0.0056,\n",
      "        -0.0296,  0.0278,  0.0142,  0.0471, -0.0239,  0.0407,  0.0266, -0.0361],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0008, -0.0845, -0.0991, -0.0911, -0.0447,  0.0079,  0.0326,  0.1160,\n",
      "          0.0403, -0.0499, -0.1215, -0.0329, -0.0357, -0.1041,  0.1150, -0.0348,\n",
      "          0.0243,  0.0797, -0.0598, -0.1228,  0.1045, -0.0874,  0.0250, -0.0405,\n",
      "          0.0447,  0.0548, -0.0224,  0.0654, -0.0862, -0.0464, -0.1052,  0.1000,\n",
      "         -0.1088,  0.0950, -0.0759, -0.0379,  0.1088, -0.0438, -0.0985, -0.0570,\n",
      "         -0.0610, -0.1090,  0.0271,  0.0999,  0.0957,  0.0641, -0.1114, -0.1056,\n",
      "          0.0644, -0.0045, -0.0003, -0.1216, -0.0281,  0.0545,  0.1075, -0.0559,\n",
      "          0.0850, -0.0253, -0.0308,  0.0698,  0.0166, -0.0644,  0.0763, -0.0444],\n",
      "        [-0.0198,  0.0966,  0.0487, -0.0922, -0.0758, -0.0494, -0.0830, -0.1149,\n",
      "         -0.0607, -0.1047, -0.1060, -0.1232,  0.0757, -0.0900, -0.1244, -0.1159,\n",
      "         -0.0385,  0.0298, -0.0494, -0.0872,  0.1247,  0.1074,  0.0027,  0.0901,\n",
      "          0.0101,  0.1165,  0.0172, -0.0950,  0.0364, -0.0186,  0.0686,  0.1175,\n",
      "          0.0701,  0.0029,  0.0586, -0.0239,  0.1052, -0.0650,  0.0645,  0.0390,\n",
      "          0.0196, -0.1081,  0.0108,  0.0080, -0.0148,  0.1063,  0.0994, -0.0965,\n",
      "          0.0610,  0.0323, -0.1088,  0.0063, -0.0695, -0.1081,  0.0946,  0.0941,\n",
      "         -0.1012, -0.0094, -0.0967,  0.0627,  0.0245,  0.0631, -0.1045, -0.0549],\n",
      "        [ 0.0298, -0.0582,  0.0240,  0.0454, -0.0951,  0.0341, -0.0661,  0.0834,\n",
      "         -0.0003, -0.0878, -0.0925, -0.0731,  0.0249,  0.0076,  0.0866, -0.0255,\n",
      "         -0.0376,  0.0465, -0.0034,  0.0592, -0.0531,  0.1175, -0.1007, -0.0089,\n",
      "         -0.0532,  0.1000,  0.0612,  0.0250,  0.0006,  0.0336,  0.0484,  0.0538,\n",
      "         -0.0985, -0.0300,  0.0127, -0.1120,  0.0517, -0.0095, -0.1018, -0.1056,\n",
      "         -0.0890, -0.0843,  0.0119, -0.0458,  0.0478,  0.0468,  0.0906,  0.0224,\n",
      "          0.1046,  0.0261,  0.0578,  0.1234, -0.0933, -0.0063,  0.1155, -0.0579,\n",
      "          0.0619, -0.0193,  0.1114,  0.0682,  0.1174,  0.1138,  0.0455, -0.0201],\n",
      "        [-0.0842, -0.0360,  0.0607,  0.0365, -0.0192, -0.0138,  0.0279,  0.0286,\n",
      "         -0.0451,  0.0074,  0.0877,  0.0856,  0.0955, -0.0893,  0.0212,  0.0680,\n",
      "         -0.0995,  0.0278, -0.0411, -0.0011,  0.0457, -0.0204,  0.1101,  0.0313,\n",
      "          0.0027, -0.0968,  0.0342, -0.0545, -0.0377,  0.1105, -0.0404,  0.0239,\n",
      "          0.0270,  0.0390, -0.0827, -0.0899,  0.0208,  0.0400,  0.0610,  0.0829,\n",
      "         -0.0505, -0.0552,  0.1199,  0.0081,  0.0294,  0.1020,  0.0181,  0.0170,\n",
      "          0.0139, -0.0435, -0.0268, -0.0965,  0.0628,  0.0135,  0.1179, -0.0700,\n",
      "         -0.1244, -0.0730,  0.0760,  0.0938,  0.0221,  0.0618, -0.1065,  0.0445],\n",
      "        [-0.0664, -0.1219, -0.0734,  0.0901,  0.0851,  0.0394, -0.0048, -0.1248,\n",
      "          0.0489, -0.0367, -0.0133, -0.0879,  0.0950,  0.1156, -0.0656, -0.1010,\n",
      "          0.0028, -0.0221, -0.0781,  0.0770, -0.1114, -0.0311,  0.0748, -0.0464,\n",
      "         -0.0909,  0.0701,  0.0774, -0.0365, -0.1035,  0.1111,  0.0402, -0.0140,\n",
      "         -0.0189,  0.0043,  0.1104, -0.1223,  0.0438,  0.0242, -0.0541, -0.0487,\n",
      "          0.0982,  0.0806,  0.0637, -0.0236, -0.0731,  0.1158,  0.0735, -0.0765,\n",
      "         -0.0527, -0.0808, -0.0582, -0.0191, -0.0647,  0.0191, -0.0375,  0.0873,\n",
      "          0.0040, -0.0440,  0.0058,  0.0763, -0.0293, -0.0797,  0.0889, -0.0519],\n",
      "        [-0.0710, -0.0959,  0.0164,  0.0003, -0.0106, -0.0950, -0.0304,  0.0672,\n",
      "          0.0027, -0.0109, -0.0065,  0.0043,  0.0804,  0.0830, -0.0667, -0.0748,\n",
      "          0.0319, -0.0315,  0.0417,  0.0289, -0.0695, -0.1005,  0.1023,  0.0999,\n",
      "          0.1117,  0.0967,  0.0900, -0.0186, -0.1017,  0.0344,  0.0009, -0.0746,\n",
      "         -0.0030,  0.0450,  0.1192,  0.0847,  0.0651, -0.0184, -0.1198, -0.1148,\n",
      "          0.0937, -0.0147, -0.1165, -0.1076, -0.1091,  0.1150, -0.0352, -0.1147,\n",
      "         -0.0320, -0.0320, -0.0127, -0.1115,  0.0852,  0.0515,  0.0591,  0.0771,\n",
      "          0.1058, -0.0586,  0.1143,  0.0337,  0.0212,  0.0937,  0.0615, -0.0454],\n",
      "        [-0.1206, -0.0567, -0.0770,  0.0329,  0.0432, -0.0373,  0.0651,  0.0841,\n",
      "          0.0916,  0.0549, -0.0918,  0.0628,  0.0103,  0.0050, -0.0134, -0.0921,\n",
      "         -0.0890,  0.0564,  0.0739,  0.0185,  0.0507, -0.0286, -0.0315, -0.0046,\n",
      "          0.0658, -0.0972, -0.0791,  0.1148, -0.0663, -0.0760, -0.0630,  0.0114,\n",
      "          0.0389,  0.0374, -0.0927, -0.0277, -0.0504, -0.0992,  0.1221,  0.0067,\n",
      "         -0.1100,  0.0703,  0.0814,  0.0907,  0.0313,  0.1152, -0.1068,  0.0575,\n",
      "          0.1185, -0.0085, -0.0879,  0.0402,  0.1033,  0.1005,  0.0629, -0.1137,\n",
      "         -0.0985,  0.0110, -0.1040,  0.0758, -0.1102, -0.0027, -0.0321, -0.0882],\n",
      "        [-0.0752, -0.0089, -0.0761,  0.0910,  0.0182, -0.0833,  0.0899,  0.0559,\n",
      "          0.0270, -0.0378, -0.0205, -0.0811, -0.1084,  0.0571,  0.0569, -0.0106,\n",
      "         -0.0696,  0.0050,  0.0624, -0.0733, -0.0995, -0.0397, -0.0120,  0.0365,\n",
      "          0.0313,  0.0486, -0.1204, -0.0984, -0.0409, -0.0522,  0.0048,  0.0879,\n",
      "         -0.0762,  0.0621,  0.0408, -0.1171, -0.1132, -0.0328,  0.0765,  0.0638,\n",
      "         -0.1004, -0.0080, -0.0985, -0.0142, -0.1019, -0.0318,  0.0633,  0.1114,\n",
      "         -0.0013, -0.1052,  0.0771, -0.1075,  0.1130, -0.0838,  0.0494, -0.0143,\n",
      "         -0.0023, -0.0011,  0.0839,  0.0288, -0.0015,  0.1000,  0.0973, -0.0461],\n",
      "        [-0.0356,  0.0975, -0.0381,  0.0405,  0.0099,  0.0867,  0.0169, -0.0228,\n",
      "         -0.0677,  0.1193,  0.1153, -0.0507, -0.0583,  0.0325, -0.0791,  0.0962,\n",
      "          0.1228,  0.0486, -0.1055, -0.0962, -0.0914,  0.0753,  0.1152,  0.0574,\n",
      "         -0.0447,  0.0493,  0.0219, -0.0491,  0.1168, -0.0452, -0.1073,  0.0621,\n",
      "          0.0623,  0.0022,  0.0461, -0.0307,  0.0597, -0.1044,  0.0791, -0.1228,\n",
      "          0.0550, -0.0093, -0.0936, -0.0928, -0.0517,  0.0019,  0.0478, -0.1241,\n",
      "          0.1240,  0.0628,  0.1022, -0.0799,  0.0915, -0.0148,  0.0371,  0.1191,\n",
      "          0.0352,  0.0905,  0.0059, -0.1192,  0.1173,  0.0267,  0.0080,  0.0447],\n",
      "        [-0.0618,  0.0353,  0.0891, -0.0086,  0.0709,  0.1090,  0.0272, -0.0443,\n",
      "         -0.1007,  0.1142,  0.0606,  0.0444, -0.0222, -0.0059, -0.0775,  0.0023,\n",
      "         -0.0653, -0.0431,  0.0989,  0.0903,  0.0530, -0.0863,  0.0822, -0.1227,\n",
      "         -0.0792, -0.0253, -0.0099,  0.1083,  0.0756,  0.0321,  0.0872, -0.0535,\n",
      "          0.0465,  0.0874, -0.1133,  0.1110, -0.0891,  0.0094, -0.0864,  0.0450,\n",
      "          0.0404, -0.1168,  0.0371, -0.1102,  0.0162, -0.1053,  0.0403, -0.0646,\n",
      "         -0.0107,  0.0013, -0.0198, -0.0320, -0.0009, -0.1056, -0.1004, -0.0381,\n",
      "          0.0084, -0.0464, -0.0163,  0.0225, -0.0029, -0.0124,  0.0897,  0.0229]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0497,  0.0124,  0.1249,  0.0030,  0.1178,  0.1106,  0.1110, -0.0845,\n",
      "        -0.0486, -0.0757], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0099,  0.0338, -0.0149,  ..., -0.0614,  0.0496,  0.0372],\n",
      "        [-0.0418,  0.0303, -0.0294,  ...,  0.0404,  0.0209, -0.0068],\n",
      "        [ 0.0126, -0.0376,  0.0243,  ..., -0.0406, -0.0408,  0.0189],\n",
      "        ...,\n",
      "        [-0.0349, -0.0559,  0.0267,  ...,  0.0554, -0.0380,  0.0186],\n",
      "        [-0.0174, -0.0222, -0.0576,  ..., -0.0388,  0.0344,  0.0611],\n",
      "        [ 0.0115, -0.0240, -0.0193,  ..., -0.0504, -0.0433, -0.0525]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0476, -0.0205,  0.0407, -0.0515,  0.0580, -0.0602,  0.0106,  0.0336,\n",
      "         0.0288,  0.0452,  0.0413,  0.0099, -0.0441, -0.0404,  0.0541, -0.0079,\n",
      "        -0.0178, -0.0208, -0.0203,  0.0600,  0.0355, -0.0322, -0.0383,  0.0596,\n",
      "        -0.0607,  0.0185, -0.0304,  0.0111, -0.0483, -0.0080,  0.0376, -0.0591,\n",
      "         0.0040,  0.0167,  0.0197, -0.0184,  0.0612,  0.0359,  0.0534, -0.0554,\n",
      "         0.0461,  0.0151,  0.0290, -0.0480, -0.0085,  0.0621,  0.0106,  0.0182,\n",
      "        -0.0346,  0.0053, -0.0373, -0.0020,  0.0012,  0.0160,  0.0402, -0.0056,\n",
      "        -0.0296,  0.0278,  0.0142,  0.0471, -0.0239,  0.0407,  0.0266, -0.0361],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(128, 256)  \n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(256, 64)  \n",
    "        self.activation2 = nn.Tanh()\n",
    "        self.linear3 = nn.Linear(64, 10)    \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c8e90a0-17ba-49f7-8772-e3f510642353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.7007, 0.6001, 0.9865, 0.2452],\n",
      "        [0.0230, 0.9083, 0.0196, 0.4573]])\n",
      "\n",
      "Weight and Bias Parameters:\n",
      "weight:\n",
      "Parameter containing:\n",
      "tensor([[ 0.2640, -0.4836,  0.2523,  0.4748],\n",
      "        [ 0.3956, -0.4835,  0.0298,  0.1438],\n",
      "        [ 0.1239, -0.4689,  0.3024,  0.2441]], requires_grad=True)\n",
      "bias:\n",
      "Parameter containing:\n",
      "tensor([ 0.0383, -0.4938,  0.3773], requires_grad=True)\n",
      "\n",
      "Output Tensor:\n",
      "tensor([[ 0.2983, -0.4420,  0.5408],\n",
      "        [-0.1729, -0.8575,  0.0718]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = nn.Linear(in_features=4, out_features=3)\n",
    "input_tensor = torch.rand(2, 4)\n",
    "print('Input:')\n",
    "print(input_tensor)\n",
    "\n",
    "print('\\nWeight and Bias Parameters:')\n",
    "for name, param in linear_layer.named_parameters():\n",
    "    print(f'{name}:')\n",
    "    print(param)\n",
    "\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "\n",
    "print('\\nOutput Tensor:')\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "632001a1-16e8-4c66-8a20-84cf8389b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny model weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(tinymodel.state_dict(), \"LeNet_model_weights.pth\")\n",
    "print(\"Tiny model weights saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa98c3b-3086-4de9-959a-843e3566e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(8, 20, kernel_size=3) \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(20 * 6 * 6, 150)  \n",
    "        self.fc2 = nn.Linear(150, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de2f6273-6eca-46ba-8e1f-23fb2a2b0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33414c27-692b-4032-8e7d-8151ac635e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 8, 8)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(4, stride=2)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfbe66-4bc7-45f0-b83a-02ada4c9091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 6, 6) * 15 + 10  \n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(6)  \n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4b82c-93a5-4dc1-b284-0ad92b026c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)  \n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.2)  \n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "298f1362-a344-4e07-b188-acee7999dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet model weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), \"4_Building_Models_with_PyTorch_model_weights.pth\")\n",
    "print(\"LeNet model weights saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc6f28-c023-4d76-bcc8-73b3d3856a98",
   "metadata": {},
   "source": [
    "# PyTorch TensorBoard Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a073b-89a8-452a-a8ac-f40f43c5988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159edce-a6b1-4d99-81cf-623b3ffd1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Image datasets and image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a87af-d1a7-4d8f-a240-870a7731b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather datasets and prepare them for consumption\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4,), (0.4,))])\n",
    "\n",
    "# Store separate training and validations splits in ./data\n",
    "training_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              batch_size=8,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                batch_size=8,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=4)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81334225-7f42-46d8-a84f-0e3267f7a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img * 0.5 + 0.5       # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c8c02-3581-4a1c-9eda-f16dc418869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a batch of 4 images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)  \n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566eadd2-f797-4686-b033-41740829fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/fashion_mnist_exp_v2') \n",
    "\n",
    "writer.add_image('Six Fashion-MNIST Images', img_grid)  \n",
    "writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fbca5-904f-4161-901c-9e23006a8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 20, 3)  \n",
    "        \n",
    "        self.fc1 = nn.Linear(20 * 5 * 5, 150)  \n",
    "        self.fc2 = nn.Linear(150, 100) \n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)   \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e9ddb-0811-4160-afa8-50f265750d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(validation_loader))\n",
    "for epoch in range(3):  \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # Basic training loop\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:  \n",
    "            print('Batch {}'.format(i + 1))\n",
    "            running_vloss = 0.0\n",
    "            \n",
    "            net.eval() \n",
    "            with torch.no_grad():  \n",
    "                for j, vdata in enumerate(validation_loader, 0):\n",
    "                    vinputs, vlabels = vdata\n",
    "                    voutputs = net(vinputs)\n",
    "                    vloss = criterion(voutputs, vlabels)\n",
    "                    running_vloss += vloss.item()\n",
    "            \n",
    "            net.train()  \n",
    "            \n",
    "            avg_loss = running_loss / 500  \n",
    "            avg_vloss = running_vloss / len(validation_loader)\n",
    "            \n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                            epoch * len(training_loader) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d34d7a-73dc-4b2b-9ddb-80f549c073a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(net.state_dict(), \"5_PyTorch_TensorBoard_Support_model_weights.pth\")\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049b359-3559-4edf-ac8a-67a85f8100e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(net, images)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124d76f-e939-434f-ba23-83b1fc642e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of data and corresponding labels\n",
    "def select_n_random(data, labels, n=200):\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# Extract a random subset of data\n",
    "images, labels = select_n_random(training_set.data, training_set.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[label] for label in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ae4cf-84c9-4fc5-b921-5640e39bf0ec",
   "metadata": {},
   "source": [
    "# Training with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb579e-2f00-4ecd-8817-d252e16b3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4,), (0.4,))  \n",
    "])\n",
    "\n",
    "training_set = torchvision.datasets.FashionMNIST('./dataset', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./dataset', train=False, transform=transform, download=True)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=8, shuffle=True, num_workers=4) \n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5764168-a063-463b-ba8b-c49a7c8e810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img * 0.5 + 0.5  \n",
    "    \n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"inferno\")  \n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)  \n",
    "\n",
    "img_grid = torchvision.utils.make_grid(images, nrow=4)  \n",
    "matplotlib_imshow(img_grid, one_channel=False)  \n",
    "print('  '.join(classes[labels[j]] for j in range(len(labels))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df31c0-0b45-44b1-97bd-2643ae734c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 20, 3)  \n",
    "        self.fc1 = nn.Linear(20 * 5 * 5, 150)  \n",
    "        self.fc2 = nn.Linear(150, 100) \n",
    "        self.fc3 = nn.Linear(100, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "\n",
    "        x = x.view(x.size(0), -1)  \n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = GarmentClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240bb8f-ae22-4e6d-8c4b-6e7258ec73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "dummy_outputs = torch.rand(8, 10)  \n",
    "dummy_labels = torch.tensor([2, 7, 1, 5, 3, 8, 6, 0])  \n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {:.4f}'.format(loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5becaa69-7d12-4521-8d08-1a041d3ba2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))  # Changed optimizer from SGD to Adam with modified learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c40f96-e03f-4395-8cfb-e361c32a0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            last_loss = running_loss / 500  # Loss per batch\n",
    "            print(f'  batch {i + 1} loss: {last_loss:.4f}')  # Formatted loss output\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28335d-1608-49f1-a618-f1a0251fc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), \"6_Training_with_PyTorch_model_weights.pth\")\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0026e5-4c91-437b-87be-2d65e0ab0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  \n",
    "writer = SummaryWriter(f'runs/fashion_trainer_{timestamp}')  \n",
    "epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799e69a-106f-4385-881e-5dae54524db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10  \n",
    "\n",
    "best_vloss = float('inf')  \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch_number + 1}:')  \n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    running_vloss = 0.0\n",
    "    with torch.no_grad():  \n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)  \n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()  \n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train: {avg_loss:.4f} | valid: {avg_vloss:.4f}')  \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                       epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'model_{timestamp}_{epoch_number}.pth'  \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c424f-38d3-4126-9890-1f29946fded8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefef6a-9849-48ed-9ea5-fc5e61bfd7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3635f4b-2023-4c8b-bc44-b835bdbb0fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
